{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with parquet files\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ In this assignment, we will use the data downloaded with the module `data_manager` to create features.\n",
    "\n",
    "(11 pts total)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ This notebook assumes that price data is available to you in the environment variable `PRICE_DATA`. If you have not done so, then execute the notebook `production_2_data_engineering.ipynb` to create this data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variables using dotenv. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\yingl\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRICE_DATA=r'C:\\UFT data science program\\DSI_1\\scaling_to_production\\05_src\\data\\temp\\stock_px.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the PRICE_DATA variable\n",
    "price_data_path = os.getenv('PRICE_DATA')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the environment variable PRICE_DATA.\n",
    "Use glob to find the path of all parquet files in the directory PRICE_DATA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yingl\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "dask.config.set({'dataframe.query-planning': True})\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ticker and using Dask, do the following:\n",
    "\n",
    "Add lags for variables Close and Adj_Close.\n",
    "\n",
    "Add returns based on Adjusted Close:\n",
    "\n",
    "returns: (Adj Close / Adj Close_lag) - 1\n",
    "Add the following range:\n",
    "\n",
    "hi_lo_range: this is the day's High minus Low.\n",
    "Assign the result to dd_feat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: \"'Adj Close' column not found in DataFrame.\"\n",
      "Column names in DataFrame: Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends',\n",
      "       'Stock Splits', 'ticker', 'Close_lag_1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "try:\n",
    "    # Step 1: Read the CSV file into a Dask DataFrame\n",
    "    file_path = 'C:/UFT data science program/DSI_1/scaling_to_production/05_src/data/temp/stock_px.csv'\n",
    "    ddf = dd.read_csv(file_path)\n",
    "\n",
    "    # Step 2: Add lagged variables for Close and Adj_Close\n",
    "    lags = [1]  # You can add more lags if needed\n",
    "    for lag in lags:\n",
    "        ddf[f'Close_lag_{lag}'] = ddf['Close'].shift(lag)\n",
    "        if 'Adj Close' in ddf.columns:  # Check if 'Adj Close' column exists\n",
    "            ddf[f'Adj_Close_lag_{lag}'] = ddf['Adj Close'].shift(lag)\n",
    "        else:\n",
    "            raise KeyError(\"'Adj Close' column not found in DataFrame.\")\n",
    "\n",
    "    # Step 3: Add returns based on Adj_Close\n",
    "    if 'Adj Close' in ddf.columns:  # Check again if 'Adj Close' column exists\n",
    "        ddf['returns'] = (ddf['Adj Close'] / ddf['Adj_Close_lag_1']) - 1\n",
    "    else:\n",
    "        raise KeyError(\"'Adj Close' column not found for computing returns.\")\n",
    "\n",
    "    # Step 4: Add the hi_lo_range\n",
    "    ddf['hi_lo_range'] = ddf['High'] - ddf['Low']\n",
    "\n",
    "    # Step 5: Assign the resulting Dask DataFrame to dd_feat\n",
    "    dd_feat = ddf.compute()  # Compute to get a pandas DataFrame if needed\n",
    "\n",
    "    print(\"Operation completed successfully.\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {str(e)}\")\n",
    "    print(f\"Column names in DataFrame: {ddf.columns}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Dask data frame to a pandas data frame.\n",
    "Add a rolling average return calculation with a window of 10 days.\n",
    "Tip: Consider using .rolling(10).mean()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Adj Close' column not found in pandas DataFrame.\n",
      "                        Date       Open       High        Low      Close  \\\n",
      "0  2013-12-02 00:00:00-05:00  27.763319  27.945016  27.349049  27.399925   \n",
      "1  2013-12-03 00:00:00-05:00  27.254567  27.574352  27.189155  27.516209   \n",
      "2  2013-12-04 00:00:00-05:00  27.370848  27.596153  26.963847  27.261831   \n",
      "3  2013-12-05 00:00:00-05:00  27.261835  27.392656  27.014726  27.072870   \n",
      "4  2013-12-06 00:00:00-05:00  27.036530  27.545281  26.992922  27.436264   \n",
      "\n",
      "    Volume  Dividends  Stock Splits ticker  \n",
      "0  2404404        0.0           0.0    JCI  \n",
      "1  3774065        0.0           0.0    JCI  \n",
      "2  3876632        0.0           0.0    JCI  \n",
      "3  2503342        0.0           0.0    JCI  \n",
      "4  2879516        0.0           0.0    JCI  \n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "#  Read the CSV file into a Dask DataFrame\n",
    "file_path = 'C:/UFT data science program/DSI_1/scaling_to_production/05_src/data/temp/stock_px.csv'\n",
    "ddf = dd.read_csv(file_path)\n",
    "\n",
    "# Compute the Dask DataFrame to get a pandas DataFrame\n",
    "df = ddf.compute()\n",
    "\n",
    "#  Add a rolling average return calculation with a window of 10 days\n",
    "if 'Adj Close' in df.columns:\n",
    "    df['rolling_avg_return'] = df['Adj Close'].pct_change().rolling(10).mean()\n",
    "else:\n",
    "    print(\"'Adj Close' column not found in pandas DataFrame.\")\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please comment:\n",
    "\n",
    "+ Was it necessary to convert to pandas to calculate the moving average return?\n",
    "\n",
    "For calculating moving averages or returns, Dask is fully capable of handling these operations without converting to Pandas.\n",
    "\n",
    "\n",
    "+ Would it have been better to do it in Dask? Why?\n",
    "Yes.\n",
    "Using Dask for large-scale data operations is generally better for:\n",
    "\n",
    "Handling datasets that exceed memory capacity.\n",
    "Optimizing performance through parallel processing.\n",
    "Building scalable data processing pipelines.\n",
    "For smaller datasets or simpler tasks, Pandas might suffice and be simpler to use. When working with large data or in a distributed environment, leveraging Daskâ€™s capabilities can provide significant performance and memory management benefits.\n",
    "\n",
    "(1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "|Criteria|Complete|Incomplete|\n",
    "|---------------------|----|----|\n",
    "|Calculations         |Calculations were done correctly.|Calculations were not done correctly.|\n",
    "|Explanation of answer|Answer was concise and explained the learner's reasoning in depth.|Answer was not concise and did not explained the learner's reasoning in depth.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-1`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
